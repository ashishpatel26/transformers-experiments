{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv').dropna()\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train, val = train_test_split(train, test_size=0.13, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment: negative tweet:  How did we just get paid and still be broke as hell?! No shopping spree for me today\n",
      "sentiment: positive tweet: i no i no bt i had only been a gamer for like 2 years when i made that attempt  lol yea i luvd F1 to an extent \n",
      "sentiment: positive tweet: I love when my ipod shuffles so all the good songs are all together\n",
      "sentiment: neutral tweet:  no i mean 2moz. I`m workin` 7-1 in a bakers then 6-4 later in a pub\n",
      "sentiment: positive tweet: Lovely walk this morning with the missus; drizzle didn`t matter\n",
      "sentiment: neutral tweet:  , just dont understand what`s it got to do with me. I`m just a nice girl\n",
      "sentiment: negative tweet: getting bored of walking up and down the stairs\n",
      "sentiment: positive tweet:  have your own style. it just might work.\n",
      "sentiment: negative tweet: fighting with mum on mothers day\n",
      "sentiment: neutral tweet:  & I got too much work to do\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "for a,b,_ in zip(train.sentiment.values[:10], train.text.values[:10], train.selected_text.values[:10]):\n",
    "    print(\"sentiment:\", a, \"tweet:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broke as hell?!\n",
      "luvd\n",
      "love\n",
      "no i mean 2moz. I`m workin` 7-1 in a bakers then 6-4 later in a pub\n",
      "Lovely walk this morning with the missus; drizzle didn`t matter\n",
      ", just dont understand what`s it got to do with me. I`m just a nice girl\n",
      "getting bored of walking up and down the stairs\n",
      "it just might work.\n",
      "fighting\n",
      "I got too much work to do\n"
     ]
    }
   ],
   "source": [
    "# Target\n",
    "for _,_,c in zip(train.sentiment.values[:10], train.text.values[:10], train.selected_text.values[:10]):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append EOS token to target text, This is the standard format for T5 targets\n",
    "train['selected_text'] = train['selected_text'] + ' </s>'\n",
    "val['selected_text'] = val['selected_text'] + ' </s>'\n",
    "\n",
    "# Apply Q&A structure\n",
    "# From Appendix D in the T5 paper\n",
    "processed_input_train = (\"question: \" + train.sentiment + \" context: \" + train.text)\n",
    "processed_input_test = (\"question: \" + test.sentiment + \" context: \" + test.text)\n",
    "processed_input_val = (\"question: \" + val.sentiment + \" context: \" + val.text)\n",
    "\n",
    "# Save data as string separated by \\n (new line)\n",
    "processed_input_str_train = '\\n'.join(processed_input_train.values.tolist())\n",
    "processed_input_str_test = '\\n'.join(processed_input_test.values.tolist())\n",
    "selected_text_str_train = '\\n'.join(train['selected_text'].values.tolist())\n",
    "processed_input_str_val = '\\n'.join(processed_input_val.values.tolist())\n",
    "selected_text_str_val = '\\n'.join(val['selected_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('question: neutral context:  I`d have responded, if I were going',\n",
       " 'I`d have responded, if I were going </s>')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_input_train[0], train['selected_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question: neutral context: Last session of the day  http://twitpic.com/67ezh'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_input_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.source', 'w') as f:\n",
    "    f.write(processed_input_str_train)\n",
    "\n",
    "with open('test.source', 'w') as f:\n",
    "    f.write(processed_input_str_test)\n",
    "    \n",
    "with open('val.source', 'w') as f:\n",
    "    f.write(processed_input_str_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.target', 'w') as f:\n",
    "    f.write(selected_text_str_train)\n",
    "    \n",
    "with open('val.target', 'w') as f:\n",
    "    f.write(selected_text_str_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep the T5 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_file(tokenizer, data_path, max_length, padding='max_length', return_tensors=\"pt\"):\n",
    "    \"\"\"\n",
    "    This function reads the text files that we prepared and returns them in tokenized form.\n",
    "\n",
    "    Actually tokenizer.batch_encode_plus returns these as a list of dictionaries where \n",
    "    each dictionary contains the word piece indices among other relevant inputs for training & inference\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    with open(data_path, \"r\") as f:\n",
    "        for text in f.readlines():\n",
    "            tokenized = tokenizer.batch_encode_plus(\n",
    "                [text], max_length=max_length, padding=padding, return_tensors=return_tensors,\n",
    "            )\n",
    "            examples.append(tokenized)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    This is the T5 dataset that can read our train, test, and dev files separately\n",
    "\n",
    "    This was patterned after the SummarizationDataset from the `transformer` library's \n",
    "    summarization example (compatible with T5)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        data_dir=\"./\",\n",
    "        type_path=\"train\",\n",
    "        max_source_length=1024,\n",
    "        max_target_length=56,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Store the tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.type_path = type_path\n",
    "        # Read the source and target files for the type of file (train, test, or val)\n",
    "        self.source = encode_file(tokenizer, os.path.join(data_dir, type_path + \".source\"), max_source_length)\n",
    "        self.target = None\n",
    "        if self.type_path != \"test\":\n",
    "            self.target = encode_file(tokenizer, os.path.join(data_dir, type_path + \".target\"), max_target_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return example as a dictionary containing source_ids, src_mask, and target_ids\n",
    "        source_ids = self.source[index][\"input_ids\"].squeeze() # (1024,)\n",
    "        # We need masks for transformers to:\n",
    "        # 1) ignore padding for both the encoder and decoder stages (src_mask)\n",
    "        # 2) ignore future tokens at the decoder stage\n",
    "        src_mask = self.source[index][\"attention_mask\"].squeeze()\n",
    "\n",
    "        if self.type_path == \"test\":\n",
    "            return {\"source_ids\": source_ids, \"source_mask\": src_mask}\n",
    "\n",
    "        target_ids = self.target[index][\"input_ids\"].squeeze() # (56, )\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids}\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        The tensors are stacked together as they are yielded.\n",
    "\n",
    "        Collate function is applied to the output of a DataLoader as it is yielded.\n",
    "        \"\"\"\n",
    "        input_ids = torch.stack([x[\"source_ids\"] for x in batch]) # BS x SL\n",
    "        masks = torch.stack([x[\"source_mask\"] for x in batch]) # BS x SL\n",
    "        pad_token_id = self.tokenizer.pad_token_id\n",
    "        if self.type_path == \"test\":\n",
    "            return {\"source_ids\": source_ids, \"source_mask\": source_mask}\n",
    "\n",
    "        target_ids = torch.stack([x[\"target_ids\"] for x in batch]) # BS x SL\n",
    "        return {\"source_ids\": input_ids, \"source_mask\": masks, \"target_ids\": target_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args: argparse.Namespace):\n",
    "    \"\"\"\n",
    "    Set all the seeds to make results replicable\n",
    "    \"\"\"\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Module(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Base Transformer model that uses Pytorch Lightning as a PyTorch wrapper.\n",
    "\n",
    "    T5 specific methods are implemented in T5Trainer\n",
    "    \"\"\"\n",
    "    def __init__(self, hparams: argparse.Namespace, **config_kwargs):\n",
    "        \"Initialize a model.\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        cache_dir = self.hparams.cache_dir if self.hparams.cache_dir else None\n",
    "        self.config = AutoConfig.from_pretrained(self.hparams.model_name_or_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.hparams.model_name_or_path,\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            self.hparams.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in self.hparams.model_name_or_path), # Checkpoint is a TF format\n",
    "            config=self.config,\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "\n",
    "        # Save dataset params\n",
    "        self.dataset_kwargs: dict = dict(\n",
    "            data_dir=self.hparams.data_dir,\n",
    "            max_source_length=self.hparams.max_source_length,\n",
    "            max_target_length=self.hparams.max_target_length,\n",
    "        )\n",
    "\n",
    "    # Forward function\n",
    "    # Defines the forward pass of the module\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids, \n",
    "        attention_mask=None, \n",
    "        decoder_input_ids=None, \n",
    "        labels=None\n",
    "        ):\n",
    "        \"\"\"\n",
    "         loss (torch.FloatTensor of shape (1,), optional, returned when lm_label is provided\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "    # Data preparation\n",
    "\n",
    "    def get_dataloader(self, type_path: str, batch_size: int, \\\n",
    "                       shuffle: bool = False, num_workers: int = 24)-> DataLoader:\n",
    "        dataset = T5Dataset(self.tokenizer, type_path=type_path, **self.dataset_kwargs)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=shuffle,\n",
    "                               num_workers = num_workers)\n",
    "        return dataloader\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        dataloader = self.get_dataloader(\"train\", batch_size=self.hparams.train_batch_size, shuffle=True)\n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "            // self.hparams.gradient_accumulation_steps\n",
    "            * float(self.hparams.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self.get_dataloader(\"val\", batch_size=self.hparams.eval_batch_size)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self.get_dataloader(\"test\", batch_size=self.hparams.eval_batch_size)\n",
    "\n",
    "    # Configure optimizers\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        # Weight decay explanation:\n",
    "        # Weight decay will not be applied to \"bias\" and \"LayerNorm.weight\" parameters\n",
    "        # When training neural networks, it is common to use \"weight decay,\" where after each update,\n",
    "        # the weights are multiplied by a factor slightly less than 1.\n",
    "        # This prevents the weights from growing too large, and can be seen as gradient descent \n",
    "        # on a quadratic regularization term.\n",
    "        # https://metacademy.org/graphs/concepts/weight_decay_neural_networks\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "        # Group parameters to those that will and will not have weight decay applied\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate,\\\n",
    "                          eps=self.hparams.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    # Forward pass and calculate loss per batch (step)\n",
    "\n",
    "    def _step(self, batch, return_text=False):\n",
    "        \"\"\"\n",
    "        Runs forward pass and calculates loss per batch. Applied for training_step, and validation_step\n",
    "        \"\"\"\n",
    "        pad_token_id = self.tokenizer.pad_token_id\n",
    "        source_ids, source_mask, y = batch[\"source_ids\"], batch[\"source_mask\"], batch[\"target_ids\"]\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        labels = y[:, 1:].clone()\n",
    "        # Change pad_token_id to -100\n",
    "        labels[y[:, 1:] == pad_token_id] = -100\n",
    "        # Run forward pass and calculate loss\n",
    "        outputs = self(source_ids, attention_mask=source_mask, decoder_input_ids=y_ids, labels=labels,)\n",
    "        # Only get loss from the output since that's all we need to apply our optimizer\n",
    "        loss = outputs[0]\n",
    "        if return_text:\n",
    "            target_text = [self.tokenizer.decode(ids) for ids in y_ids]\n",
    "            return loss, target_text\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "    # Step during training\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Runs forward pass, calculates loss, and returns loss (and logs) in a dict\n",
    "        \"\"\"\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        # Notice that each training step loss is recorded on tensorboard,\n",
    "        # which makes sense since we're tracking loss per batch\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    # Adjust weights based on calculated gradients and learning rate scheduler\n",
    "\n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "        \"\"\"\n",
    "        Adjust weights based on calculated gradients + learning rate scheduler, and refresh gradients\n",
    "        \"\"\"\n",
    "        optimizer.step()\n",
    "\n",
    "        # Refresh gradients (to zero)\n",
    "        optimizer.zero_grad()\n",
    "        # Update the learning rate scheduler\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "    # Step during validation\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Runs forward pass, calculates loss, and returns loss in a dict\n",
    "        \"\"\"\n",
    "\n",
    "        # Return source and target text to calculate jaccard score only for validation\n",
    "        loss, target_text = self._step(batch, return_text=True)\n",
    "\n",
    "        preds = self.test_step(batch, batch_idx)\n",
    "        preds_text = preds[\"preds\"]\n",
    "        # Track jaccard score to get validation accuracy\n",
    "        jaccard_score = [jaccard(p, t) for p, t in zip(preds_text, target_text)]\n",
    "\n",
    "        return {\"val_loss\": loss, \"jaccard_score\": jaccard_score}\n",
    "\n",
    "    # Show loss after validation\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Calculate average loss for all the validation batches\n",
    "        \"\"\"\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        jaccard_scores = sum([x[\"jaccard_score\"] for x in outputs], [])\n",
    "        avg_jaccard_score = np.mean(jaccard_scores)\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss, \"jaccard_score\": avg_jaccard_score}\n",
    "        return {\"avg_val_loss\": avg_loss, \"avg_jaccard_score\": avg_jaccard_score, \"log\": tensorboard_logs}\n",
    "\n",
    "    # Step during testing\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Runs forward pass on test set and returns calculated loss, predictions, and targets\n",
    "        Note: this assumes that your test set has targets (doesn't have for kaggle).\n",
    "        \"\"\"\n",
    "        pad_token_id = self.tokenizer.pad_token_id\n",
    "        source_ids, source_mask = batch[\"source_ids\"], batch[\"source_mask\"]\n",
    "        # NOTE: the following kwargs get more speed and lower quality summaries than those in evaluate_cnn.py\n",
    "        # For the sentiment span extraction task, turning off early stopping proved superior\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=source_ids,\n",
    "            attention_mask=source_mask,\n",
    "            num_beams=1,\n",
    "            max_length=80,\n",
    "            repetition_penalty=2.5,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        preds = [\n",
    "            self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            for g in generated_ids\n",
    "        ]\n",
    "\n",
    "        return {\"preds\": preds}\n",
    "\n",
    "    # Note: we don't attempt to print the loss from the test set, \n",
    "    # because it's assumed that we don't have the test targets\n",
    "    def test_end(self, outputs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        for pred in outputs:\n",
    "            preds += pred[\"preds\"]\n",
    "        return {\"preds\": preds}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Save test predictions and targets as text files and return the calculated loss for the test set\n",
    "        \"\"\"\n",
    "        output_test_predictions_file = os.path.join(self.hparams.output_dir, \"test_predictions.txt\")\n",
    "        # write predictions and targets for later rouge evaluation.\n",
    "        with open(output_test_predictions_file, \"w+\") as p_writer:\n",
    "            for output_batch in outputs:\n",
    "                p_writer.writelines(s + \"\\n\" for s in output_batch[\"preds\"])\n",
    "            p_writer.close()\n",
    "\n",
    "        return self.test_end(outputs)\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        \"\"\"\n",
    "        Print average loss and learning rate at each step\n",
    "        \"\"\"\n",
    "        avg_loss = getattr(self.trainer, \"avg_loss\", 0.0)\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "        return tqdm_dict\n",
    "\n",
    "    def _feature_file(self, mode):\n",
    "        return os.path.join(\n",
    "            self.hparams.data_dir,\n",
    "            \"cached_{}_{}_{}\".format(\n",
    "                mode,\n",
    "                list(filter(None, self.hparams.model_name_or_path.split(\"/\"))).pop(),\n",
    "                str(self.hparams.max_seq_length),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def is_logger(self):\n",
    "        return True # for single GPU\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parser, root_dir):\n",
    "        parser.add_argument(\n",
    "            \"--model_name_or_path\",\n",
    "            default=None,\n",
    "            type=str,\n",
    "            required=True,\n",
    "            help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--config_name\", default=\"\", type=str, help=\"Pretrained config \\\n",
    "            name or path if not the same as model_name\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--tokenizer_name\",\n",
    "            default=\"\",\n",
    "            type=str,\n",
    "            help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--cache_dir\",\n",
    "            default=\"\",\n",
    "            type=str,\n",
    "            help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
    "        )\n",
    "        parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning\\\n",
    "        rate for Adam.\")\n",
    "        parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "        parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "        parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "        parser.add_argument(\n",
    "            \"--num_train_epochs\", default=3, type=int, help=\"Total number of training epochs to perform.\"\n",
    "        )\n",
    "\n",
    "        parser.add_argument(\"--train_batch_size\", default=32, type=int)\n",
    "        parser.add_argument(\"--eval_batch_size\", default=32, type=int)\n",
    "\n",
    "        parser.add_argument(\n",
    "            \"--max_source_length\",\n",
    "            default=1024,\n",
    "            type=int,\n",
    "            help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--max_target_length\",\n",
    "            default=56,\n",
    "            type=int,\n",
    "            help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\",\n",
    "        )\n",
    "\n",
    "        parser.add_argument(\n",
    "            \"--data_dir\",\n",
    "            default=None,\n",
    "            type=str,\n",
    "            required=True,\n",
    "            help=\"The input data dir. Should contain the dataset files for the text generation task.\",\n",
    "        )\n",
    "        return parser\n",
    "\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
    "        logger.info(\"***** Validation results *****\")\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            # Log results\n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "\n",
    "def add_generic_args(parser, root_dir):\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--fp16\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--fp16_opt_level\",\n",
    "        type=str,\n",
    "        default=\"O1\",\n",
    "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--n_gpu\", type=int, default=1)\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to run predictions on the test set.\")\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
    "\n",
    "\n",
    "def generic_train(model: T5Module, args: argparse.Namespace):\n",
    "    # init model\n",
    "    set_seed(args)\n",
    "\n",
    "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n",
    "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "\n",
    "    # Can take out checkpoint saving after each epoch to save memory\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
    "    )\n",
    "\n",
    "    train_params = dict(\n",
    "        accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "        gpus=args.n_gpu,\n",
    "        max_epochs=args.num_train_epochs,\n",
    "        gradient_clip_val=args.max_grad_norm,\n",
    "        checkpoint_callback=checkpoint_callback,\n",
    "        callbacks=[LoggingCallback()],\n",
    "        enable_pl_optimizer=False\n",
    "    )\n",
    "\n",
    "    if args.fp16:\n",
    "        train_params[\"use_amp\"] = args.fp16\n",
    "        train_params[\"amp_level\"] = args.fp16_opt_level\n",
    "\n",
    "    if args.n_gpu > 1:\n",
    "        train_params[\"distributed_backend\"] = \"ddp\"\n",
    "\n",
    "    trainer = pl.Trainer(**train_params)\n",
    "\n",
    "    if args.do_train:\n",
    "        trainer.fit(model)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "    # If output_dir not provided, a folder will be generated in pwd\n",
    "    if not args.output_dir:\n",
    "        args.output_dir = os.path.join(\"./results\", f\"{args.task}_{time.strftime('%Y%m%d_%H%M%S')}\",)\n",
    "        os.makedirs(args.output_dir)\n",
    "    model = T5Module(args)\n",
    "    trainer = generic_train(model, args)\n",
    "\n",
    "    # Save the last model as model.bin\n",
    "    #checkpoints = list(sorted(glob.glob(os.path.join(args.output_dir, \"checkpointepoch=*.ckpt\"), recursive=True)))\n",
    "    #model = model.load_from_checkpoint(checkpoints[-1])\n",
    "    model.model.save_pretrained(args.output_dir)\n",
    "    # Save tokenizer files\n",
    "    model.tokenizer.save_pretrained('./')\n",
    "    \n",
    "    # Optionally, predict on dev set and write to output_dir\n",
    "    if args.do_predict:\n",
    "        # See https://github.com/huggingface/transformers/issues/3159\n",
    "        # pl use this format to create a checkpoint:\n",
    "        # https://github.com/PyTorchLightning/pytorch-lightning/blob/master\\\n",
    "        # /pytorch_lightning/callbacks/model_checkpoint.py#L169\n",
    "        trainer.test(model)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘output’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True, used: True\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/mirac13/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: When overriding `LightningModule` optimizer_step with `Trainer(..., enable_pl_optimizer=False, ...)`, we won't be calling `.zero_grad` we can't assume when you call your `optimizer.step()`. For Lightning to take care of it, please use `Trainer(enable_pl_optimizer=True)`.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "INFO:lightning:\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:***** Validation results *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9d7dffde6445c3af330bdf2781ac15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "optimizer_step() got an unexpected keyword argument 'on_tpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-50a8069fd85a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_model_specific_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARGS_STR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-c4ef01e11e5a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneric_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Save the last model as model.bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f38eacd288bc>\u001b[0m in \u001b[0;36mgeneric_train\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_fit_start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# train or test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_or_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtrain_or_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;31m# run train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0;31m# ------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m                 \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0;31m# when returning -1 from train_step, we end epoch early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                         \u001b[0;31m# optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step_and_backward_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# model hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         model_ref.optimizer_step(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: optimizer_step() got an unexpected keyword argument 'on_tpu'"
     ]
    }
   ],
   "source": [
    "ARGS_STR = \"\"\"\n",
    "--data_dir=./ \\\n",
    "--model_name_or_path=t5-base \\\n",
    "--learning_rate=3e-5 \\\n",
    "--train_batch_size=32 \\\n",
    "--output_dir=output/ \\\n",
    "--do_train \\\n",
    "--n_gpu=1 \\\n",
    "--num_train_epochs 5 \\\n",
    "--max_source_length 80 \\\n",
    "\"\"\"\n",
    "#\n",
    "#--eval_batch_size=3 \\\n",
    "#--do_predict \\\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "add_generic_args(parser, os.getcwd())\n",
    "parser = T5Module.add_model_specific_args(parser, os.getcwd())\n",
    "args = parser.parse_args(ARGS_STR.split())\n",
    "trainer = main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
