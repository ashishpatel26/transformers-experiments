{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv').dropna()\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train, val = train_test_split(train, test_size=0.13, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment: negative tweet:  How did we just get paid and still be broke as hell?! No shopping spree for me today\n",
      "sentiment: positive tweet: i no i no bt i had only been a gamer for like 2 years when i made that attempt  lol yea i luvd F1 to an extent \n",
      "sentiment: positive tweet: I love when my ipod shuffles so all the good songs are all together\n",
      "sentiment: neutral tweet:  no i mean 2moz. I`m workin` 7-1 in a bakers then 6-4 later in a pub\n",
      "sentiment: positive tweet: Lovely walk this morning with the missus; drizzle didn`t matter\n",
      "sentiment: neutral tweet:  , just dont understand what`s it got to do with me. I`m just a nice girl\n",
      "sentiment: negative tweet: getting bored of walking up and down the stairs\n",
      "sentiment: positive tweet:  have your own style. it just might work.\n",
      "sentiment: negative tweet: fighting with mum on mothers day\n",
      "sentiment: neutral tweet:  & I got too much work to do\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "for a,b,_ in zip(train.sentiment.values[:10], train.text.values[:10], train.selected_text.values[:10]):\n",
    "    print(\"sentiment:\", a, \"tweet:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broke as hell?!\n",
      "luvd\n",
      "love\n",
      "no i mean 2moz. I`m workin` 7-1 in a bakers then 6-4 later in a pub\n",
      "Lovely walk this morning with the missus; drizzle didn`t matter\n",
      ", just dont understand what`s it got to do with me. I`m just a nice girl\n",
      "getting bored of walking up and down the stairs\n",
      "it just might work.\n",
      "fighting\n",
      "I got too much work to do\n"
     ]
    }
   ],
   "source": [
    "# Target\n",
    "for _,_,c in zip(train.sentiment.values[:10], train.text.values[:10], train.selected_text.values[:10]):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append EOS token to target text, This is the standard format for T5 targets\n",
    "train['selected_text'] = train['selected_text'] + ' </s>'\n",
    "val['selected_text'] = val['selected_text'] + ' </s>'\n",
    "\n",
    "# Apply Q&A structure\n",
    "# From Appendix D in the T5 paper\n",
    "processed_input_train = (\"question: \" + train.sentiment + \" context: \" + train.text)\n",
    "processed_input_test = (\"question: \" + test.sentiment + \" context: \" + test.text)\n",
    "processed_input_val = (\"question: \" + val.sentiment + \" context: \" + val.text)\n",
    "\n",
    "# Save data as string separated by \\n (new line)\n",
    "processed_input_str_train = '\\n'.join(processed_input_train.values.tolist())\n",
    "processed_input_str_test = '\\n'.join(processed_input_test.values.tolist())\n",
    "selected_text_str_train = '\\n'.join(train['selected_text'].values.tolist())\n",
    "processed_input_str_val = '\\n'.join(processed_input_val.values.tolist())\n",
    "selected_text_str_val = '\\n'.join(val['selected_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('question: neutral context:  I`d have responded, if I were going',\n",
       " 'I`d have responded, if I were going </s>')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_input_train[0], train['selected_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question: neutral context: Last session of the day  http://twitpic.com/67ezh'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_input_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.source', 'w') as f:\n",
    "    f.write(processed_input_str_train)\n",
    "\n",
    "with open('test.source', 'w') as f:\n",
    "    f.write(processed_input_str_test)\n",
    "    \n",
    "with open('val.source', 'w') as f:\n",
    "    f.write(processed_input_str_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.target', 'w') as f:\n",
    "    f.write(selected_text_str_train)\n",
    "    \n",
    "with open('val.target', 'w') as f:\n",
    "    f.write(selected_text_str_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep the T5 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_file(tokenizer, data_path, max_length, padding='max_length', return_tensors=\"pt\"):\n",
    "    \"\"\"\n",
    "    This function reads the text files that we prepared and returns them in tokenized form.\n",
    "\n",
    "    Actually tokenizer.batch_encode_plus returns these as a list of dictionaries where \n",
    "    each dictionary contains the word piece indices among other relevant inputs for training & inference\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    with open(data_path, \"r\") as f:\n",
    "        for text in f.readlines():\n",
    "            tokenized = tokenizer.batch_encode_plus(\n",
    "                [text], max_length=max_length, padding=padding, return_tensors=return_tensors,\n",
    "            )\n",
    "            examples.append(tokenized)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    This is the T5 dataset that can read our train, test, and dev files separately\n",
    "\n",
    "    This was patterned after the SummarizationDataset from the `transformer` library's \n",
    "    summarization example (compatible with T5)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        data_dir=\"./\",\n",
    "        type_path=\"train\",\n",
    "        max_source_length=1024,\n",
    "        max_target_length=56,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Store the tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.type_path = type_path\n",
    "        # Read the source and target files for the type of file (train, test, or val)\n",
    "        self.source = encode_file(tokenizer, os.path.join(data_dir, type_path + \".source\"), max_source_length)\n",
    "        self.target = None\n",
    "        if self.type_path != \"test\":\n",
    "            self.target = encode_file(tokenizer, os.path.join(data_dir, type_path + \".target\"), max_target_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return example as a dictionary containing source_ids, src_mask, and target_ids\n",
    "        source_ids = self.source[index][\"input_ids\"].squeeze() # (1024,)\n",
    "        # We need masks for transformers to:\n",
    "        # 1) ignore padding for both the encoder and decoder stages (src_mask)\n",
    "        # 2) ignore future tokens at the decoder stage\n",
    "        src_mask = self.source[index][\"attention_mask\"].squeeze()\n",
    "\n",
    "        if self.type_path == \"test\":\n",
    "            return {\"source_ids\": source_ids, \"source_mask\": src_mask}\n",
    "\n",
    "        target_ids = self.target[index][\"input_ids\"].squeeze() # (56, )\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids}\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        The tensors are stacked together as they are yielded.\n",
    "\n",
    "        Collate function is applied to the output of a DataLoader as it is yielded.\n",
    "        \"\"\"\n",
    "        input_ids = torch.stack([x[\"source_ids\"] for x in batch]) # BS x SL\n",
    "        masks = torch.stack([x[\"source_mask\"] for x in batch]) # BS x SL\n",
    "        pad_token_id = self.tokenizer.pad_token_id\n",
    "        if self.type_path == \"test\":\n",
    "            return {\"source_ids\": source_ids, \"source_mask\": source_mask}\n",
    "\n",
    "        target_ids = torch.stack([x[\"target_ids\"] for x in batch]) # BS x SL\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": source_mask, \"target_ids\": y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
