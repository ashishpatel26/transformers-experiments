{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nlp\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "from typing import Dict, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data.dataset import Dataset, IterableDataset\n",
    "from transformers import PreTrainedTokenizer, DataCollator, PreTrainedModel\n",
    "from transformers import AutoTokenizer, EvalPrediction, Trainer, HfArgumentParser, TrainingArguments, \\\n",
    "    AutoModelForSequenceClassification, set_seed, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5df4216f8a46bbb130a511e4bd7620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=28940.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023da3181bbf4941b52213fb17c88f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=30329.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset glue/mnli (download: 298.29 MiB, generated: 78.65 MiB, post-processed: Unknown sizetotal: 376.95 MiB) to /home/mirac13/.cache/huggingface/datasets/glue/mnli/1.0.0/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299fb8d20ebc45b1b6f2396d5a8ca55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=312783507.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/mirac13/.cache/huggingface/datasets/glue/mnli/1.0.0/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4. Subsequent calls will reuse this data.\n",
      "{'train': Dataset(features: {'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, num_rows: 392702), 'validation_matched': Dataset(features: {'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, num_rows: 9815), 'validation_mismatched': Dataset(features: {'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, num_rows: 9832), 'test_matched': Dataset(features: {'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, num_rows: 9796), 'test_mismatched': Dataset(features: {'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, num_rows: 9847)}\n",
      "{'hypothesis': 'Product and geography are what make cream skimming work. ', 'idx': 0, 'label': 1, 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}\n"
     ]
    }
   ],
   "source": [
    "dataset = nlp.load_dataset('glue', 'mnli')\n",
    "\n",
    "# Let's get an idea of the data format\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Example:\n",
    "    text_a: str\n",
    "    text_b: str\n",
    "    label: int\n",
    "\n",
    "# to simplify code below, we convert list of dict provided by nlp package to list of Example\n",
    "train = [Example(text_a=item[\"premise\"], text_b=item[\"hypothesis\"], \\\n",
    "                 label=item[\"label\"]) for item in dataset[\"train\"]]\n",
    "valid = [Example(text_a=item[\"premise\"], text_b=item[\"hypothesis\"], \\\n",
    "                 label=item[\"label\"]) for item in dataset[\"validation_matched\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic padding\n",
    "On MNLI, shortest sequences are < 20 tokens long, if you set the max length to 512 tokens, you will add 492 pad tokens to those 20 tokens sequences, and then perform computations over those 492 noisy tokens.\n",
    "\n",
    "Because the learning / gradient descent is performed at the mini batch level, we have the opportunity to limit the padding effect, more precisely we can first search for the longest sequence length in the mini batch, and then pad the other sequences accordingly.\n",
    "\n",
    "Those operations can be performed in the collate_fn function.\n",
    "\n",
    "Below, we define a custom Dataset class which doesn't perform any padding (if asked so) and a custom collate_fn (in DataCollator class) which will perform the dynamic padding when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Features:\n",
    "    input_ids: List[int]\n",
    "    attention_mask: List[int]\n",
    "    label: int\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, \n",
    "                 max_len: int,\n",
    "                 examples: List[Example]) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.examples: List[Example] = examples\n",
    "        self.current = 0\n",
    "\n",
    "    def encode(self, ex: Example) -> Features:\n",
    "        encode_dict = self.tokenizer.encode_plus(text=ex.text_a,\n",
    "                                                 text_pair=ex.text_b,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 max_length=self.max_len,\n",
    "                                                 padding = 'max_length',\n",
    "                                                 return_token_type_ids=False,\n",
    "                                                 return_attention_mask=True,\n",
    "                                                 return_overflowing_tokens=False,\n",
    "                                                 return_special_tokens_mask=False,\n",
    "                                                 )\n",
    "        return Features(input_ids=encode_dict[\"input_ids\"],\n",
    "                        attention_mask=encode_dict[\"attention_mask\"],\n",
    "                        label=ex.label)\n",
    "\n",
    "    def __getitem__(self, idx) -> Features:\n",
    "        return self.encode(ex=self.examples[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "\n",
    "def pad_seq(seq: List[int], max_batch_len: int, pad_value: int) -> List[int]:\n",
    "    return seq + (max_batch_len - len(seq)) * [pad_value]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SmartCollator():\n",
    "    pad_token_id: int\n",
    "\n",
    "    def __call__(self, batch: List[Features]) -> Dict[str, torch.Tensor]:\n",
    "        batch_inputs = list()\n",
    "        batch_attention_masks = list()\n",
    "        labels = list()\n",
    "        max_size = max([len(ex.input_ids) for ex in batch])\n",
    "        for item in batch:\n",
    "            batch_inputs += [pad_seq(item.input_ids, max_size, self.pad_token_id)]\n",
    "            batch_attention_masks += [pad_seq(item.attention_mask, max_size, 0)]\n",
    "            labels.append(item.label)\n",
    "\n",
    "        return {\"input_ids\": torch.tensor(batch_inputs, dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(batch_attention_masks, dtype=torch.long),\n",
    "                \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "                }\n",
    "\n",
    "def load_transformers_model(pretrained_model_name_or_path: str,\n",
    "                            use_cuda: bool,\n",
    "                            ) -> PreTrainedModel:\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8a8429267844acabcda56bed3bebd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=433.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b9738a041c40e4891c64d444085937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=213450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba19b57042b48138c7d256a9cf7bb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435797.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239f58a6e05e439eaca402408a4d6fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435779157.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_sequence_len = 512 # longest sequences are >> 256 tokens, we choose to not apply any truncation.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"bert-base-cased\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name_or_path=\"bert-base-cased\",\n",
    "                                    num_labels=3)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"bert-base-cased\",\n",
    "    config=config)\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"acc\": (preds == p.label_ids).mean()}\n",
    "\n",
    "args = TrainingArguments(output_dir=\"/tmp/test_dynamic_padding\",\n",
    "                         seed=42,\n",
    "                         num_train_epochs=1,\n",
    "                         per_device_train_batch_size=8,  \n",
    "                         # max batch size without OOM exception, because of the large max token length\n",
    "                         per_device_eval_batch_size=8,\n",
    "                         logging_steps=5000,\n",
    "                         save_steps=0,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
