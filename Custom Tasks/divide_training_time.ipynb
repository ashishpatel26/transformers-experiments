{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nlp\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "from typing import Dict, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data.dataset import Dataset, IterableDataset\n",
    "from transformers import PreTrainedTokenizer, DataCollator, PreTrainedModel\n",
    "from transformers import AutoTokenizer, EvalPrediction, Trainer, HfArgumentParser, TrainingArguments, \\\n",
    "    AutoModelForSequenceClassification, set_seed, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset(features: {'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, num_rows: 392702), 'validation_matched': Dataset(features: {'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, num_rows: 9815), 'validation_mismatched': Dataset(features: {'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, num_rows: 9832), 'test_matched': Dataset(features: {'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, num_rows: 9796), 'test_mismatched': Dataset(features: {'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, num_rows: 9847)}\n",
      "{'hypothesis': 'Product and geography are what make cream skimming work. ', 'idx': 0, 'label': 1, 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}\n"
     ]
    }
   ],
   "source": [
    "dataset = nlp.load_dataset('glue', 'mnli')\n",
    "\n",
    "# Let's get an idea of the data format\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Example:\n",
    "    text_a: str\n",
    "    text_b: str\n",
    "    label: int\n",
    "\n",
    "# to simplify code below, we convert list of dict provided by nlp package to list of Example\n",
    "train = [Example(text_a=item[\"premise\"], text_b=item[\"hypothesis\"], \\\n",
    "                 label=item[\"label\"]) for item in dataset[\"train\"]]\n",
    "valid = [Example(text_a=item[\"premise\"], text_b=item[\"hypothesis\"], \\\n",
    "                 label=item[\"label\"]) for item in dataset[\"validation_matched\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic padding\n",
    "On MNLI, shortest sequences are < 20 tokens long, if you set the max length to 512 tokens, you will add 492 pad tokens to those 20 tokens sequences, and then perform computations over those 492 noisy tokens.\n",
    "\n",
    "Because the learning / gradient descent is performed at the mini batch level, we have the opportunity to limit the padding effect, more precisely we can first search for the longest sequence length in the mini batch, and then pad the other sequences accordingly.\n",
    "\n",
    "Those operations can be performed in the collate_fn function.\n",
    "\n",
    "Below, we define a custom Dataset class which doesn't perform any padding (if asked so) and a custom collate_fn (in DataCollator class) which will perform the dynamic padding when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Features:\n",
    "    input_ids: List[int]\n",
    "    attention_mask: List[int]\n",
    "    label: int\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, \n",
    "                 max_len: int,\n",
    "                 padding: Union[str, bool],\n",
    "                 examples: List[Example]) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.examples: List[Example] = examples\n",
    "        self.current = 0\n",
    "        self.padding = padding\n",
    "\n",
    "    def encode(self, ex: Example) -> Features:\n",
    "        encode_dict = self.tokenizer.encode_plus(text=ex.text_a,\n",
    "                                                 text_pair=ex.text_b,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 max_length=self.max_len,\n",
    "                                                 padding = self.padding,\n",
    "                                                 return_token_type_ids=False,\n",
    "                                                 return_attention_mask=True,\n",
    "                                                 return_overflowing_tokens=False,\n",
    "                                                 return_special_tokens_mask=False,\n",
    "                                                 )\n",
    "        return Features(input_ids=encode_dict[\"input_ids\"],\n",
    "                        attention_mask=encode_dict[\"attention_mask\"],\n",
    "                        label=ex.label)\n",
    "\n",
    "    def __getitem__(self, idx) -> Features:\n",
    "        return self.encode(ex=self.examples[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "\n",
    "def pad_seq(seq: List[int], max_batch_len: int, pad_value: int) -> List[int]:\n",
    "    return seq + (max_batch_len - len(seq)) * [pad_value]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SmartCollator():\n",
    "    pad_token_id: int\n",
    "\n",
    "    def __call__(self, batch: List[Features]) -> Dict[str, torch.Tensor]:\n",
    "        batch_inputs = list()\n",
    "        batch_attention_masks = list()\n",
    "        labels = list()\n",
    "        max_size = max([len(ex.input_ids) for ex in batch])\n",
    "        for item in batch:\n",
    "            batch_inputs += [pad_seq(item.input_ids, max_size, self.pad_token_id)]\n",
    "            batch_attention_masks += [pad_seq(item.attention_mask, max_size, 0)]\n",
    "            labels.append(item.label)\n",
    "\n",
    "        return {\"input_ids\": torch.tensor(batch_inputs, dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(batch_attention_masks, dtype=torch.long),\n",
    "                \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "                }\n",
    "\n",
    "def load_transformers_model(pretrained_model_name_or_path: str,\n",
    "                            use_cuda: bool,\n",
    "                            ) -> PreTrainedModel:\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "max_sequence_len = 256 # longest sequences are >> 256 tokens, we choose to not apply any truncation.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"bert-base-cased\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name_or_path=\"bert-base-cased\",\n",
    "                                    num_labels=3)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"bert-base-cased\",\n",
    "    config=config)\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"acc\": (preds == p.label_ids).mean()}\n",
    "\n",
    "args = TrainingArguments(output_dir=\"/tmp/test_dynamic_padding\",\n",
    "                         seed=42,\n",
    "                         num_train_epochs=1,\n",
    "                         per_device_train_batch_size=8,  \n",
    "                         # max batch size without OOM exception, because of the large max token length\n",
    "                         per_device_eval_batch_size=8,\n",
    "                         logging_steps=5000,\n",
    "                         save_steps=0,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training without dynamic padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='49088' max='49088' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49088/49088 1:29:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.821699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.711096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.678438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.644936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.619195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.595900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.572465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.547119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.529884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 89.57mn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1227' max='1227' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1227/1227 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5191093683242798, 'eval_acc': 0.8008150789607743, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# to disable dynamic padding, we just pad to max at the dataset level.\n",
    "train_set = TextDataset(tokenizer=tokenizer,\n",
    "                        max_len=max_sequence_len,\n",
    "                        examples=train,\n",
    "                        padding = 'max_length')  \n",
    "\n",
    "valid_set = TextDataset(tokenizer=tokenizer,\n",
    "                        max_len=max_sequence_len,\n",
    "                        examples=valid,\n",
    "                        padding = 'max_length')  \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_set,\n",
    "    data_collator=SmartCollator(pad_token_id=tokenizer.pad_token_id),\n",
    "    eval_dataset=valid_set,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "print(f\"training took {(time.time() - start_time) / 60:.2f}mn\")\n",
    "result = trainer.evaluate()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with dynamic padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='49088' max='49088' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49088/49088 24:42:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.577070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.573953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.567290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.552262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.538078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.526835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.526117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.510929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.513998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 1482.40mn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1227' max='1227' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1227/1227 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5283517241477966, 'eval_acc': 0.7994905756495161, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "train_set = TextDataset(tokenizer=tokenizer,\n",
    "                        max_len=max_sequence_len,\n",
    "                        examples=train,\n",
    "                        padding = False)  \n",
    "\n",
    "valid_set = TextDataset(tokenizer=tokenizer,\n",
    "                        max_len=max_sequence_len,\n",
    "                        examples=valid,\n",
    "                        padding = False)  \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_set,\n",
    "    data_collator=SmartCollator(pad_token_id=tokenizer.pad_token_id),\n",
    "    eval_dataset=valid_set,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "print(f\"training took {(time.time() - start_time) / 60:.2f}mn\")\n",
    "result = trainer.evaluate()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
