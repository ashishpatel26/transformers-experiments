{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict\n",
    "from torch.utils.data import Dataset\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "from captum.attr import (IntegratedGradients, LayerIntegratedGradients,\n",
    "                         configure_interpretable_embedding_layer,\n",
    "                         remove_interpretable_embedding_layer)\n",
    "from transformers import (ElectraForSequenceClassification,\n",
    "                          ElectraTokenizerFast, EvalPrediction, InputFeatures,\n",
    "                          Trainer, TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model and Tokenizer\n",
    "\n",
    "model = ElectraForSequenceClassification.from_pretrained(\n",
    "    \"google/electra-small-discriminator\", num_labels = 2)\n",
    "\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\n",
    "    \"google/electra-small-discriminator\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/mirac13/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set labels: {0, 1}\n",
      "Validation set labels: {0, 1}\n",
      "Test set labels: {-1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates something rather beautiful about human nature</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same throughout</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the filmmakers could dredge up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     senence  \\\n",
       "0  hide new secretions from the parental units                                                 \n",
       "1  contains no wit , only labored gags                                                         \n",
       "2  that loves its characters and communicates something rather beautiful about human nature    \n",
       "3  remains utterly satisfied to remain the same throughout                                     \n",
       "4  on the worst revenge-of-the-nerds clichés the filmmakers could dredge up                    \n",
       "\n",
       "   label  \n",
       "0  0      \n",
       "1  0      \n",
       "2  1      \n",
       "3  0      \n",
       "4  0      "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the SST2 dataset from the datasets library\n",
    "dataset = datasets.load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Load the SST2 metric from the datasets library\n",
    "glue_metric = datasets.load_metric('glue', 'sst2')\n",
    "\n",
    "# Look at the labels\n",
    "print(\"Training set labels: {}\".format(set(dataset[\"train\"][\"label\"])))\n",
    "print(\"Validation set labels: {}\".format(set(dataset[\"validation\"][\"label\"])))\n",
    "print(\"Test set labels: {}\".format(set(dataset[\"test\"][\"label\"])))\n",
    "\n",
    "# Explore the dataset\n",
    "df = pd.DataFrame({\"senence\": dataset[\"train\"][\"sentence\"],\n",
    "                   \"label\": dataset[\"train\"][\"label\"]})\n",
    "pd.options.display.max_colwidth = 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset class\n",
    "\n",
    "class TrainerDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.tokenized_inputs = tokenizer(inputs, padding=True)   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return InputFeatures(\n",
    "            input_ids=self.tokenized_inputs['input_ids'][idx],\n",
    "            token_type_ids=self.tokenized_inputs['token_type_ids'][idx],\n",
    "            attention_mask=self.tokenized_inputs['attention_mask'][idx],\n",
    "            label=self.targets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainerDataset(dataset[\"train\"][\"sentence\"],\n",
    "                               dataset[\"train\"][\"label\"], tokenizer)\n",
    "eval_dataset = TrainerDataset(dataset[\"validation\"][\"sentence\"],\n",
    "                              dataset[\"validation\"][\"label\"], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"electra_sst2\",\n",
    "    num_train_epochs=3,  # (1 epoch gives slightly lower accuracy)\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=64,    \n",
    "    dataloader_drop_last=True,  # Make sure all batches are of equal size\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    # The choice of a dataset (task_name) implies metric\n",
    "    return glue_metric.compute(\n",
    "        task_name=\"sst-2\",\n",
    "        preds=preds,\n",
    "        labels=p.label_ids)\n",
    "\n",
    "\n",
    "# Instantiate the Trainer class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3156' max='3156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3156/3156 03:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.123000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3156, training_loss=0.19133564909148126, metrics={'train_runtime': 182.5762, 'train_samples_per_second': 17.286, 'total_flos': 1083750877034496, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: {'accuracy': 0.911348872755063}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "for obj in eval_dataset:\n",
    "    preds = model(input_ids = torch.LongTensor([obj.input_ids]).to(device), \\\n",
    "                  token_type_ids = torch.LongTensor([obj.token_type_ids]).to(device), \\\n",
    "                  attention_mask = torch.LongTensor([obj.attention_mask]).to(device))\n",
    "    preds = torch.LongTensor([np.argmax(preds.logits.to('cpu').detach().numpy())]).to(device)\n",
    "    glue_metric.add_batch(predictions = preds, references = torch.LongTensor([obj.label]).to(device))\n",
    "    \n",
    "print(\"Accuracy: {}\".format(glue_metric.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability\n",
    "\n",
    "\n",
    "The examples below use two attribution methods from the Captum library:\n",
    "\n",
    "- **Integrated Gradients** - the method requires configuring interpretation hooks to perform attribution for all three embedding layers in one step, and\n",
    "- **Layer Integrated Gradients**, computed separately with respect to each of the three layers:\n",
    "     - model.electra.embeddings.word_embeddings,\n",
    "     - model.electra.embeddings.token_type_embeddings,\n",
    "     - model.electra.embeddings.position_embeddings.\n",
    "     \n",
    "We will try to find out to what extent, according to these methods, each token has contributed to the model's prediction, or, more precisely, to its shift from the baseline output. Each method requires setting a target class index: 0 for negative or 1 for a positive sentiment. Attribution is performed for each target class separately. Scores will be assigned with regard to the model's output for the selected class.\n",
    "\n",
    "The shape of attributions is the same as the shape of the inputs parameter of the attribute method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"visually imaginative , thematically instructive and thoroughly \\\n",
    "delightful , it takes us on a roller-coaster ride from innocence to experience \\\n",
    "without even a hint of that typical kiddie-flick sentimentality . \"\n",
    "true_label = 1\n",
    "\n",
    "[x for x in dataset[\"validation\"] if x[\"sentence\"] == text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "The functions below construct input tensors for our sample and for a sequence of [PAD] tokens serving as a baseline. We also need to define a forward function running inference on the model. The function will be passed on to objects handling attribution.\n",
    "\n",
    "Computation with IntegratedGradients requires altering the model by configuring additional layers. For this purpose, the Captum library provides the configure_interpretable_embedding_layer and remove_interpretable_embedding_layer methods. Configuring an interpretable embedding layer modifies the model. A model with interpretable layers requires input of a different shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_interpretable_embeddings():\n",
    "    \"\"\"Configure interpretable embedding layer\"\"\"\n",
    "    interpretable_embedding1 = configure_interpretable_embedding_layer(\n",
    "        model, \"electra.embeddings.word_embeddings\")\n",
    "    interpretable_embedding2 = configure_interpretable_embedding_layer(\n",
    "        model, \"electra.embeddings.token_type_embeddings\")\n",
    "    interpretable_embedding3 = configure_interpretable_embedding_layer(\n",
    "        model,\"electra.embeddings.position_embeddings\")\n",
    "    return (interpretable_embedding1,\n",
    "            interpretable_embedding2,\n",
    "            interpretable_embedding3)\n",
    "\n",
    "\n",
    "def remove_interpretable_embeddings(interpretable_embedding1, \n",
    "                                    interpretable_embedding2, \n",
    "                                    interpretable_embedding3):\n",
    "    \"\"\"Remove interpretable layer to restore the original model structure\"\"\"\n",
    "    if not \\\n",
    "    type(model.get_input_embeddings()).__name__ == \"InterpretableEmbeddingBase\":\n",
    "        return\n",
    "    remove_interpretable_embedding_layer(model, interpretable_embedding1)\n",
    "    remove_interpretable_embedding_layer(model, interpretable_embedding2)\n",
    "    remove_interpretable_embedding_layer(model, interpretable_embedding3)  \n",
    "\n",
    "\n",
    "def predict_forward_func(input_ids, token_type_ids=None, \n",
    "                         position_ids=None, attention_mask=None):\n",
    "    \"\"\"Function passed to ig constructors\"\"\"\n",
    "    return model(input_ids=input_ids, \n",
    "                 token_type_ids=token_type_ids, \n",
    "                 position_ids=position_ids, \n",
    "                 attention_mask=attention_mask)[0]  \n",
    "\n",
    "\n",
    "def prepare_input(text):\n",
    "    \"\"\"Prepare ig attribution input: tokenize sample and baseline text.\"\"\"\n",
    "    tokenized_text = tokenizer(text, return_tensors=\"pt\", \n",
    "                               return_attention_mask=True)\n",
    "    seq_len = tokenized_text[\"input_ids\"].shape[1]\n",
    "    position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    # Construct the baseline (a reference sample).\n",
    "    # A sequence of [PAD] tokens of length equal to that of the processed sample\n",
    "    ref_text = tokenizer.pad_token * (seq_len - 2) # special tokens\n",
    "    tokenized_ref_text = tokenizer(ref_text, return_tensors=\"pt\") \n",
    "    ref_position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    return (tokenized_text[\"input_ids\"],\n",
    "            tokenized_text[\"token_type_ids\"], \n",
    "            position_ids,\n",
    "            tokenized_ref_text[\"input_ids\"],\n",
    "            tokenized_ref_text[\"token_type_ids\"], \n",
    "            ref_position_ids,\n",
    "            tokenized_text[\"attention_mask\"])   \n",
    "\n",
    "\n",
    "def prepare_input_embed(input_ids, token_type_ids, position_ids,\n",
    "                        ref_input_ids, ref_token_type_ids, ref_position_ids,\n",
    "                        attention_mask):\n",
    "    \"\"\"Construct input for the modified model\"\"\"\n",
    "    input_ids_embed = interpretable_embedding1.indices_to_embeddings(input_ids)\n",
    "    ref_input_ids_embed = interpretable_embedding1.indices_to_embeddings(\n",
    "        ref_input_ids)\n",
    "    token_type_ids_embed = interpretable_embedding2.indices_to_embeddings(\n",
    "        token_type_ids)\n",
    "    ref_token_type_ids_embed = interpretable_embedding2.indices_to_embeddings(\n",
    "        ref_token_type_ids)\n",
    "    position_ids_embed = interpretable_embedding3.indices_to_embeddings(\n",
    "        position_ids)\n",
    "    ref_position_ids_embed = interpretable_embedding3.indices_to_embeddings(\n",
    "        ref_position_ids)\n",
    "    \n",
    "    return (input_ids_embed, token_type_ids_embed, position_ids_embed, \n",
    "            ref_input_ids_embed, ref_token_type_ids_embed, \n",
    "            ref_position_ids_embed, attention_mask)\n",
    "\n",
    "\n",
    "def get_input_data(text):\n",
    "    input_data = place_on_device(*prepare_input(text))\n",
    "    input_data_embed = prepare_input_embed(*input_data)   \n",
    "    return input_data, input_data_embed \n",
    "\n",
    "\n",
    "def place_on_device(*tensors):\n",
    "    tensors_device = []\n",
    "    for t in tensors:\n",
    "        tensors_device.append(t.to(device))\n",
    "    return tuple(tensors_device)  \n",
    "\n",
    "\n",
    "def ig_attribute(ig, class_index, input_data_embed):\n",
    "    return ig.attribute(inputs=input_data_embed[0:3],\n",
    "                        baselines=input_data_embed[3:6],\n",
    "                        additional_forward_args=(input_data_embed[6]),\n",
    "                        target = class_index,\n",
    "                        return_convergence_delta=True,\n",
    "                        n_steps=200)\n",
    "    \n",
    "\n",
    "def lig_attribute(lig, class_index, input_data):\n",
    "    return lig.attribute(\n",
    "        inputs=input_data[0], baselines=input_data[3],\n",
    "        additional_forward_args=(input_data[1], input_data[2], input_data[6]),\n",
    "        return_convergence_delta=True, target=class_index, n_steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated Gradients\n",
    "To compute attributions with Integrated Gradients we will:\n",
    "\n",
    "- instantiate the IntegratedGradients class passing the predict_forward_func function as parameter,\n",
    "- configure interpretable embedding layer,\n",
    "- prepare input tensors,\n",
    "- compute attributions,\n",
    "- remove interpratable embedding layer.\n",
    "- Calling the get_input_embeddings method of the model helps to find out whether extra layers have been configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the IntegratedGradients class\n",
    "ig = IntegratedGradients(predict_forward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure interpretable embedding layer \n",
    "print(\"Original model's input embeddings:\\n {}\\n\".\n",
    "      format(model.get_input_embeddings()))\n",
    "if not \\\n",
    "type(model.get_input_embeddings()).__name__ == \"InterpretableEmbeddingBase\":\n",
    "    interpretable_embedding1, interpretable_embedding2,\\\n",
    "    interpretable_embedding3 = configure_interpretable_embeddings()\n",
    "print(\"Input embeddings with interpretable layer:\\n {}\\n\".\n",
    "      format(model.get_input_embeddings()))\n",
    "\n",
    "# Prepare input\n",
    "input_data, input_data_embed = get_input_data(text)  \n",
    "\n",
    "# Compute attributions for both target classes\n",
    "# class 0 (negative)\n",
    "attributions_0, approximation_error_0 = ig_attribute(ig, 0, input_data_embed)\n",
    "# class 1 (positive)\n",
    "attributions_1, approximation_error_1 = ig_attribute(ig, 1, input_data_embed)\n",
    "\n",
    "# Remove interpratable embedding layer used by ig attribution\n",
    "remove_interpretable_embeddings(interpretable_embedding1, \n",
    "                                interpretable_embedding2, \n",
    "                                interpretable_embedding3)\n",
    "print(\"\\nInput embeddings with interpretable layer removed:\\n {}\\n\"\n",
    ".format(model.get_input_embeddings()))\n",
    "\n",
    "print(\"\\nThe reference sample:\\n{}\".format(tokenizer.convert_ids_to_tokens(\n",
    "    input_data[3].clone().detach().to('cpu').numpy().squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness\n",
    "The Integrated Gradients method satisfies the completeness property. The sum of attributions should be equal, with certain accuracy, to the difference between the model's output for the sample and its output for the selected baseline (in this case a sequence of [PAD] tokens). Increase the n_steps parameter of the attribute method to obtain better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_completeness(attributions_0, attributions_1):\n",
    "    input_ids, token_type_ids, position_ids, ref_input_ids,\\\n",
    "    ref_token_type_ids, ref_position_ids, attention_mask = input_data\n",
    "\n",
    "    # Prediction for the sample\n",
    "    scores = predict_forward_func(input_ids, token_type_ids,\n",
    "                                position_ids, attention_mask) \n",
    "    # Prediction for the baseline\n",
    "    ref_scores = predict_forward_func(ref_input_ids, ref_token_type_ids,\n",
    "                                    ref_position_ids, attention_mask)\n",
    "\n",
    "    # Put on cpu\n",
    "    if torch.is_tensor(attributions_0[0]):\n",
    "        attributions_0 = [x.clone().detach().to('cpu').numpy() \n",
    "        for x in attributions_0]\n",
    "    if torch.is_tensor(attributions_1[0]):\n",
    "        attributions_1 = [x.clone().detach().to('cpu').numpy() \n",
    "        for x in attributions_1]  \n",
    "    scores = scores.clone().detach().to('cpu').numpy().squeeze()\n",
    "    ref_scores = ref_scores.clone().detach().to('cpu').numpy().squeeze()    \n",
    "\n",
    "    # How prediction for the sample differs from baseline prediction  \n",
    "    diff_from_baseline = scores - ref_scores\n",
    "\n",
    "    # Sum of attributions\n",
    "    attributions_sum0 = [x.sum() for x in attributions_0]\n",
    "    attributions_sum1 = [x.sum() for x in attributions_1]\n",
    "    attributions_sum = [sum(attributions_sum0), sum(attributions_sum1)]\n",
    "\n",
    "    # Difference from the baseline output for both classes\n",
    "    diff = diff_from_baseline - attributions_sum\n",
    "\n",
    "    # Find out which layers contribute to the score \n",
    "    print(\"Class 0: input tokens attr. sum: {}\".format(attributions_sum0[0]))\n",
    "    print(\"Classs 0: token type attr. sum: {}\".format(attributions_0[1].sum()))\n",
    "    print(\"Class 0: position ids attr. sum: {}\".format(attributions_0[2].sum()))\n",
    "    print(\"Class 1: input tokens attr. sum: {}\".format(attributions_1[0].sum()))\n",
    "    print(\"Classs 1: token type attr. sum: {}\".format(attributions_1[1].sum()))\n",
    "    print(\"Class 1: position ids attr. sum: {}\".format(attributions_1[2].sum()))\n",
    "\n",
    "    # Compare sum of attributions with the difference from baseline prediction\n",
    "    print(\"\\nPrediction for sample: {}\".format(scores))\n",
    "    print(\"Prediction for baseline: {}\".format(ref_scores))\n",
    "    print(\"Difference from baseline: {}\".format(diff_from_baseline))\n",
    "    print(\"Sum of attributions: {}\".format(attributions_sum))\n",
    "    print(\"\\nClass 0:\\n score: {}\\n reference score: {}\\\n",
    "    \\n difference from ref.: {}\\n sum of attributions:  {}\\\n",
    "    \\n difference from reference - attributions: {}\".\\\n",
    "    format(scores[0], ref_scores[0], diff_from_baseline[0], \n",
    "            attributions_sum[0], diff[0]))\n",
    "    print(\"\\nClass 1:\\n score: {}\\n reference score: {}\\\n",
    "    \\n difference from ref.: {}\\n sum of attributions:  {}\\\n",
    "    \\n difference from reference - attributions: {}\".\\\n",
    "    format(scores[1], ref_scores[1], diff_from_baseline[1], \n",
    "            attributions_sum[1], diff[1]))\n",
    "    \n",
    "    return attributions_0, attributions_1\n",
    "    \n",
    "    \n",
    "attributions_0, attributions_1 = check_completeness(attributions_0,\n",
    "                                                    attributions_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Integrated Gradients\n",
    "With Layer Integrated Gradients, attributions are computed with respect to a certain layer. We'll run the algorithm for three layers separately:\n",
    "\n",
    "- model.electra.embeddings.word_embeddings\n",
    "- model.electra.embeddings.token_type_embeddings\n",
    "- model.electra.embeddings.position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for lig attributions (model with no special layers configured)\n",
    "input_data = place_on_device(*prepare_input(text))\n",
    "\n",
    "# 1. Layer: model.electra.embeddings.word_embeddings\n",
    "lig_we = LayerIntegratedGradients(\n",
    "    predict_forward_func, \n",
    "    model.electra.embeddings.word_embeddings)\n",
    "layer_attributions_we_0, _ = lig_attribute(lig_we, 0, input_data)\n",
    "layer_attributions_we_1, _ = lig_attribute(lig_we, 1, input_data)\n",
    "\n",
    "# 2. Layer: model.electra.embeddings.token_type_embeddings\n",
    "lig_tte = LayerIntegratedGradients(\n",
    "    predict_forward_func,\n",
    "    model.electra.embeddings.token_type_embeddings)\n",
    "layer_attributions_tte_0, _ = lig_attribute(lig_tte, 0, input_data)\n",
    "layer_attributions_tte_1, _ = lig_attribute(lig_tte, 1, input_data)\n",
    "\n",
    "# 3. Layer: model.electra.embeddings.position_embeddings\n",
    "lig_pe = LayerIntegratedGradients(\n",
    "    predict_forward_func, \n",
    "    model.electra.embeddings.position_embeddings)\n",
    "layer_attributions_pe_0, _ = lig_attribute(lig_pe, 0, input_data)\n",
    "layer_attributions_pe_1, _ = lig_attribute(lig_pe, 1, input_data)\n",
    "\n",
    "print(\"Shape of attributions:\")\n",
    "print(layer_attributions_we_0.shape, layer_attributions_we_1.shape)\n",
    "print(layer_attributions_tte_0.shape, layer_attributions_tte_1.shape)\n",
    "print(layer_attributions_pe_0.shape, layer_attributions_pe_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completeness\n",
    "Completeness for attributions found for each layer separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_attributions_0, layer_attributions_1 = check_completeness(\n",
    "    (layer_attributions_we_0, layer_attributions_tte_0, layer_attributions_pe_0),\n",
    "    (layer_attributions_we_1, layer_attributions_tte_1, layer_attributions_pe_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with IG\n",
    "\n",
    "# Attributions for input_ids computed with IG and LIG\n",
    "# word_embeddings: index 0\n",
    "ig_1 = attributions_1[0].squeeze().sum(1)\n",
    "lig_1 = layer_attributions_1[0].squeeze().sum(1)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer(text)[\"input_ids\"])\n",
    "\n",
    "range_ig = [x + 0.5 for x in np.arange(len(ig_1))]\n",
    "range_lig = [x + 0.5 for x in range_ig]\n",
    " \n",
    "plt.rcParams[\"figure.figsize\"] = [12, 6] \n",
    "plt.bar(range_ig, ig_1, width=0.5, label='ig')\n",
    "plt.bar(range_lig, lig_1, width=0.5, label='lig')\n",
    "plt.xlabel('Token', fontweight='bold')\n",
    "plt.xticks(list(range(len(lig_1))), tokens, rotation='vertical')\n",
    "plt.legend()\n",
    "plt.title(\"Attributions with IG and LIG for the positive target class.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for both target classes\n",
    "# Attributions assigned to tokens may take opposite values when computed with regard to class 0 and class 1.\n",
    "\n",
    "# Attributions for word_embeddings: index 0\n",
    "lig_0 = layer_attributions_0[0].squeeze().sum(1)\n",
    "lig_1 = layer_attributions_1[0].squeeze().sum(1)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer(text)[\"input_ids\"])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 6]\n",
    "plt.bar(list(range(len(lig_0))), lig_0, color='r', alpha=0.5)\n",
    "plt.bar(list(range(len(lig_1))), lig_1, color='g', alpha=0.5)\n",
    "plt.xticks(list(range(len(lig_0))), tokens, rotation='vertical')\n",
    "plt.legend(labels=[\"Target: negative\", \"Target: positive\"])\n",
    "plt.xlabel('Token', fontweight='bold')\n",
    "plt.title(\"Token attributions for positive and negative target class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions\n",
    "\n",
    "\n",
    "def compute_attributions_ig(ig, input_data_embed):\n",
    "    # Create interpretable layer\n",
    "    if not type(\n",
    "        model.get_input_embeddings()).__name__ == \"InterpretableEmbeddingBase\":    \n",
    "        interpretable_embedding1, interpretable_embedding2,\\\n",
    "        interpretable_embedding3 = configure_interpretable_embeddings()\n",
    "    # Compute attributions for positive and nagative samples (class 1 and 0)\n",
    "    attr_0, delta_0 = ig_attribute(ig, 0, input_data_embed)\n",
    "    attr_1, delta_1 = ig_attribute(ig, 1, input_data_embed)\n",
    "    # Remove interprateble layer used by ig attribution\n",
    "    remove_interpretable_embeddings(interpretable_embedding1, \n",
    "                                    interpretable_embedding2, \n",
    "                                    interpretable_embedding3)\n",
    "    # Return sum over all three layers\n",
    "    attr_0 = torch.stack(attr_0, axis=0).sum(0)\n",
    "    attr_1 = torch.stack(attr_1, axis=0).sum(0)    \n",
    "    return (attr_0, delta_0), (attr_1, delta_1)    \n",
    "\n",
    "\n",
    "def compute_attributions_lig(lig, input_data):  \n",
    "    # Compute attributions for positive and nagative samples (class 1 and 0)\n",
    "    return lig_attribute(lig, 0, input_data), lig_attribute(lig, 1, input_data)\n",
    "\n",
    "\n",
    "def get_visualization_record(text, attributions, scores, true_label,\n",
    "                             all_tokens, approximation_error):\n",
    "    attributions_sum = summarize_attributions(attributions)\n",
    "    return viz.VisualizationDataRecord(\n",
    "        attributions_sum,\n",
    "        torch.max(torch.softmax(scores[0], dim=0)),\n",
    "        torch.argmax(scores),\n",
    "        true_label,\n",
    "        text,\n",
    "        attributions_sum.sum(),\n",
    "        all_tokens,\n",
    "        approximation_error)\n",
    "    \n",
    "\n",
    "def visualize_attributions(text, true_label, ig_object, \n",
    "                           method, layer_name=None):\n",
    "    # Prepare input\n",
    "    input_data, input_data_embed = get_input_data(text)\n",
    "\n",
    "    # Compute attributions\n",
    "    attr_0, attr_1, delta_0, delta_1 = None, None, None, None\n",
    "    if method == \"ig\":\n",
    "        (attr_0, delta_0), (attr_1, delta_1) = \\\n",
    "        compute_attributions_ig(ig_object, input_data_embed)\n",
    "    elif method == \"lig\":    \n",
    "        (attr_0, delta_0), (attr_1, delta_1) = \\\n",
    "        compute_attributions_lig(ig_object, input_data)\n",
    "    else:\n",
    "        return \"method: ig or lig\"    \n",
    "    # Run inference\n",
    "    scores = predict_forward_func(*input_data[0:3], input_data[-1])\n",
    "    # Prepare visualization \n",
    "    indices = input_data[0][0].detach().tolist()\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "    data_vis_0 = get_visualization_record(text, attr_0, scores, \n",
    "                                          true_label, all_tokens, delta_0)  \n",
    "    data_vis_1 = get_visualization_record(text, attr_1, scores, \n",
    "                                          true_label, all_tokens, delta_1) \n",
    "    # Visualize\n",
    "    print(\"\\nAttribution method: {},\".\n",
    "          format(method), \"class index: 0 (negative)\")\n",
    "    if not layer_name is None:\n",
    "        print(\"Layer: {}\".format(layer_name))\n",
    "    viz.visualize_text([data_vis_0])\n",
    "    print(\"Attribution method: {},\".\n",
    "          format(method), \"Class index: 1 (positive)\")\n",
    "    if not layer_name is None:\n",
    "        print(\"Layer: {}\".format(layer_name))    \n",
    "    viz.visualize_text([data_vis_1])\n",
    "     \n",
    "    return attr_0, attr_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples\n",
    "Captum visualization library shows in green tokens that push the prediction towards the target class. Those driving the score towards the reference value are marked in red. As a result, words perceived as positive will appear in green if attribution is performed against class 1 (positive) but will be highlighted in red with an attribution targeting class 0 (negative).\n",
    "\n",
    "Because importance scores ar assigned to tokens, not words, some examples may show, that attribution is highly dependent on tokenization. Classification results may vary between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "eval_pred_result = trainer.predict(eval_dataset)\n",
    "predictions = np.argmax(eval_pred_result.predictions, axis=1)\n",
    "\n",
    "# Find correctly classified and misclassifed samples\n",
    "eval_samples = [tokenizer.decode(x.input_ids, skip_special_tokens=True) \\\n",
    "                for x in eval_dataset]\n",
    "eval_preds = list(zip(eval_pred_result.label_ids, predictions))\n",
    "positive_pred_as_positive = [sample for sample, (real_label, pred_label) \\\n",
    "                             in zip(eval_samples, eval_preds) \\\n",
    "                             if real_label == pred_label and real_label == 1]  \n",
    "negative_pred_as_negative = [sample for sample, (real_label, pred_label) \\\n",
    "                             in zip(eval_samples, eval_preds) \\\n",
    "                             if real_label == pred_label and real_label == 0] \n",
    "positive_pred_as_negative = [sample for sample, (real_label, pred_label) \\\n",
    "                             in zip(eval_samples, eval_preds) \\\n",
    "                             if real_label != pred_label and real_label == 1]                                                           \n",
    "negative_pred_as_positive = [sample for sample, (real_label, pred_label) \\\n",
    "                             in zip(eval_samples, eval_preds) \\\n",
    "                             if real_label != pred_label and real_label == 0]\n",
    "\n",
    "# Browse\n",
    "print('\\n'.join(positive_pred_as_positive))   \n",
    "print('\\n'.join(negative_pred_as_negative))    \n",
    "print('\\n'.join(positive_pred_as_negative))  \n",
    "print('\\n'.join(negative_pred_as_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive\n",
    "# A correctly classified positive sample\n",
    "\n",
    "text_vis = text\n",
    "true_label_vis = true_label\n",
    "\n",
    "ig_0, ig_1 = visualize_attributions(text_vis, true_label_vis, ig, \"ig\")\n",
    "lig_0, lig_1 = \\\n",
    "visualize_attributions(text_vis, true_label_vis, lig_we, \"lig\",\n",
    "                       layer_name=\"electra.embeddings.word_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative\n",
    "# A correctly classified negative sample\n",
    "\n",
    "text_vis = 'the film makes a fatal mistake : it asks us to care about a young \\\n",
    "man whose only apparent virtue is that he is not quite as unpleasant as some \\\n",
    "of the people in his life.'\n",
    "true_label_vis = 0\n",
    "\n",
    "ig_0, ig_1 = visualize_attributions(text_vis, true_label_vis, ig, \"ig\")\n",
    "lig_0, lig_1 = \\\n",
    "visualize_attributions(text_vis, true_label_vis, lig_we, \"lig\",\n",
    "                       layer_name=\"electra.embeddings.word_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misclassified\n",
    "# Pick an example. Results may vary between runs.\n",
    "\n",
    "print('-------------------------------Negative examples misclassified as positive------------------------------')\n",
    "print('\\n\\n'.join(negative_pred_as_positive))\n",
    "\n",
    "print('-------------------------------Positive examples misclassified as negative------------------------------')\n",
    "print('\\n\\n'.join(positive_pred_as_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vis = \"a tv style murder mystery with a few big screen moments ( including one that seems to be made for a different film altogether ).\"\n",
    "true_label_vis = 0\n",
    "\n",
    "ig_0, ig_1 = visualize_attributions(text_vis, true_label_vis, ig, \"ig\")\n",
    "lig_0, lig_1 = \\\n",
    "visualize_attributions(text_vis, true_label_vis, lig_we, \"lig\",\n",
    "                       layer_name=\"electra.embeddings.word_embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
