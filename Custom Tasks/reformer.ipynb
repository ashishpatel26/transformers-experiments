{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nlp\n",
    "import torch\n",
    "import psutil\n",
    "import pyarrow\n",
    "import humanize\n",
    "import warnings\n",
    "\n",
    "import GPUtil as GPU\n",
    "\n",
    "from transformers import (\n",
    "    ReformerModelWithLMHead,\n",
    "    ReformerTokenizer,\n",
    "    ReformerConfig,\n",
    "    Trainer,\n",
    "    DataCollator,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPUs = GPU.getGPUs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = GPUs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen RAM Free: 29.5 GB  | Proc size: 327.4 MB\n",
      "GPU RAM Free: 9540MB | Used: 470MB | Util   5% | Total 10010MB\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ),\\\n",
    "      \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(\\\n",
    "        gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nlp.load_dataset(\"crime_and_punish\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ReformerTokenizer.from_pretrained(\"google/reformer-crime-and-punishment\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Because want to pack all data into a single sample, we use the handy map() function to reduce the dataset into one sample and pad the sample to a length of 524288. We then expand the same sample to 8 training samples so that we can accumulate gradients during training. Finally, we make the dataset ready for training, by only keeping the columns needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 2 ** 19  # 524288\n",
    "\n",
    "# define our map function to reduce the dataset to one sample\n",
    "def flatten_and_tokenize(batch):\n",
    "  all_input_text = [\"\".join(batch[\"line\"])]\n",
    "  input_ids_dict = tokenizer.batch_encode_plus(\n",
    "      all_input_text, padding = 'max_length', max_length=sequence_length\n",
    "  )\n",
    "\n",
    "    # duplicate data 8 times to have have 8 examples in dataset\n",
    "  for key in input_ids_dict.keys():\n",
    "    input_ids_dict[key] = [8 * [x] for x in input_ids_dict[key]][0]\n",
    "\n",
    "  return input_ids_dict\n",
    "\n",
    "# reduce the dataset and set batch_size to all inputs\n",
    "dataset = dataset.map(\n",
    "  flatten_and_tokenize, batched=True, batch_size=-1, remove_columns=[\"line\"]\n",
    ")\n",
    "\n",
    "# prepare dataset to be in torch format\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(features: {'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}, num_rows: 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want the model to just memorize the dataset (the single example) by encoding the words in its position embeddings. Thus, at each training iteration we will randomly select how much padding to put before the text vs. after it.\n",
    "\n",
    "With the Trainer framework of transformers, we can implement by using a Reformer specific DataCollator that randomely shifts the input_ids to the right and sets the labels correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReformerCollator:\n",
    "    def __init__(self, max_roll_length):\n",
    "        self.max_roll_length = max_roll_length\n",
    "\n",
    "    def collate_batch(self, features):\n",
    "        # get random shift int\n",
    "        random_shift_length = torch.randint(self.max_roll_length, (1,)).item()\n",
    "\n",
    "        # shift input and mask\n",
    "        rolled_input_ids = torch.roll(\n",
    "            features[0][\"input_ids\"], random_shift_length\n",
    "        ).unsqueeze(0)\n",
    "        rolled_attention_mask = torch.roll(\n",
    "            features[0][\"attention_mask\"], random_shift_length\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": rolled_input_ids,  # BS x SEQ_LEN\n",
    "            \"labels\": rolled_input_ids,  # BS x SEQ_LEN\n",
    "            \"attention_mask\": rolled_attention_mask,  # BS x SEQ_LEN\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instantiate the data collator the length of padded input_ids needs to be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the non_padded_sequence_length defines the max shift for our data collator\n",
    "non_padded_sequence_length = sequence_length - sum(\n",
    "    dataset[\"attention_mask\"][0]\n",
    ")\n",
    "\n",
    "# get the data collator\n",
    "data_collator = ReformerCollator(non_padded_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define our reformer model by defining the ReformerConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"attention_head_size\": 64,\n",
    "    \"attn_layers\": [\"local\", \"lsh\", \"local\", \"lsh\", \"local\", \"lsh\"],\n",
    "    \"axial_pos_embds\": True,\n",
    "    \"sinusoidal_pos_embds\": False,\n",
    "    \"axial_pos_embds_dim\": [64, 192],\n",
    "    \"axial_pos_shape\": [512, 1024],\n",
    "    \"lsh_attn_chunk_length\": 64,\n",
    "    \"local_attn_chunk_length\": 64,\n",
    "    \"feed_forward_size\": 512,\n",
    "    \"hidden_act\": \"relu\",\n",
    "    \"hidden_size\": 256,\n",
    "    \"is_decoder\": True,\n",
    "    \"max_position_embeddings\": 524288,\n",
    "    \"num_attention_heads\": 2,\n",
    "    \"num_buckets\": [64, 128],\n",
    "    \"num_hashes\": 1,\n",
    "    \"vocab_size\": 320,\n",
    "    \"lsh_attention_probs_dropout_prob\": 0.0,\n",
    "    \"lsh_num_chunks_before\": 1,\n",
    "    \"lsh_num_chunks_after\": 0,\n",
    "    \"local_num_chunks_before\": 1,\n",
    "    \"local_num_chunks_after\": 0,\n",
    "    \"local_attention_probs_dropout_prob\": 0.025,\n",
    "    \"hidden_dropout_prob\": 0.025,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ReformerConfig(**config)\n",
    "model = ReformerModelWithLMHead(config)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training args\n",
    "\n",
    "training_args = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"max_steps\": 2000,\n",
    "    \"do_train\": True,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"logging_steps\": 50,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"fp16\": True,\n",
    "    \"per_gpu_train_batch_size\": 1,\n",
    "    \"per_gpu_eval_batch_size\": 1,\n",
    "    \"save_steps\": 50,\n",
    "    \"output_dir\": \"./\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(**training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accuracy metrics\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    non_padded_indices = (pred.label_ids != -100)\n",
    "\n",
    "    # correctly shift labels and pred as it's done in forward()\n",
    "    labels = pred.label_ids[..., 1:][non_padded_indices[..., 1:]]\n",
    "    pred = np.argmax(pred.predictions[:, :-1], axis=-1)[non_padded_indices[..., :-1]]\n",
    "\n",
    "    acc = np.mean(np.asarray(pred == labels), dtype=np.float)\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
