{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "reformer.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTk_WuSZzEOS"
      },
      "source": [
        "!pip -q install gputil psutil humanize transformers nlp sentencepiece"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBbmqC6Lz--X"
      },
      "source": [
        "import transformers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOD3UbI1zYey"
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6ZJsGXLy4V0"
      },
      "source": [
        "import os\n",
        "import nlp\n",
        "import torch\n",
        "import psutil\n",
        "import pyarrow\n",
        "import humanize\n",
        "import warnings\n",
        "\n",
        "import GPUtil as GPU\n",
        "\n",
        "from transformers import (\n",
        "    ReformerModelWithLMHead,\n",
        "    ReformerTokenizer,\n",
        "    ReformerConfig,\n",
        "    Trainer,\n",
        "    DataCollator,\n",
        "    TrainingArguments,\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0yWIjJTy4V3"
      },
      "source": [
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxq_hxATy4V3"
      },
      "source": [
        "GPUs = GPU.getGPUs()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SfC3BaMy4V3",
        "outputId": "1042eec4-1b73-48c7-c9c9-66353a5947a3"
      },
      "source": [
        "GPUs"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<GPUtil.GPUtil.GPU at 0x7fdd40dec550>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN2NF8EozycI"
      },
      "source": [
        "gpu = GPUs[0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1L6ZHy_y4V4",
        "outputId": "b3c57114-9fcd-4f31-c1d2-769855245f91"
      },
      "source": [
        "process = psutil.Process(os.getpid())\n",
        "print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ),\\\n",
        "      \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(\\\n",
        "        gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.6 GB  | Proc size: 538.1 MB\n",
            "GPU RAM Free: 16270MB | Used: 10MB | Util   0% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9B-ZZ9hy4V4"
      },
      "source": [
        "dataset = nlp.load_dataset(\"crime_and_punish\", split=\"train\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAMtTQLoy4V4"
      },
      "source": [
        "tokenizer = ReformerTokenizer.from_pretrained(\"google/reformer-crime-and-punishment\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1UJ9Qj2y4V5"
      },
      "source": [
        "Because want to pack all data into a single sample, we use the handy map() function to reduce the dataset into one sample and pad the sample to a length of 524288. We then expand the same sample to 8 training samples so that we can accumulate gradients during training. Finally, we make the dataset ready for training, by only keeping the columns needed for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaSLVzgCy4V5"
      },
      "source": [
        "sequence_length = 2 ** 19 \n",
        "\n",
        "# define our map function to reduce the dataset to one sample\n",
        "def flatten_and_tokenize(batch):\n",
        "  all_input_text = [\"\".join(batch[\"line\"])]\n",
        "  input_ids_dict = tokenizer.batch_encode_plus(\n",
        "      all_input_text, padding = 'max_length', max_length=sequence_length\n",
        "  )\n",
        "\n",
        "    # duplicate data 8 times to have have 8 examples in dataset\n",
        "  for key in input_ids_dict.keys():\n",
        "    input_ids_dict[key] = [8 * [x] for x in input_ids_dict[key]][0]\n",
        "\n",
        "  return input_ids_dict\n",
        "\n",
        "# reduce the dataset and set batch_size to all inputs\n",
        "dataset = dataset.map(\n",
        "  flatten_and_tokenize, batched=True, batch_size=-1, remove_columns=[\"line\"]\n",
        ")\n",
        "\n",
        "# prepare dataset to be in torch format\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ0ys8sry4V5",
        "outputId": "4b7fbf37-946c-46a3-8565-8c2752cd4bb4"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset(features: {'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}, num_rows: 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-e55gIny4V5"
      },
      "source": [
        "We don't want the model to just memorize the dataset (the single example) by encoding the words in its position embeddings. Thus, at each training iteration we will randomly select how much padding to put before the text vs. after it.\n",
        "\n",
        "With the Trainer framework of transformers, we can implement by using a Reformer specific DataCollator that randomely shifts the input_ids to the right and sets the labels correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyf9aLEMy4V5"
      },
      "source": [
        "class ReformerCollator:\n",
        "    def __init__(self, max_roll_length):\n",
        "        self.max_roll_length = max_roll_length\n",
        "\n",
        "    def __call__(self, features):\n",
        "        # get random shift int\n",
        "        random_shift_length = torch.randint(self.max_roll_length, (1,)).item()\n",
        "\n",
        "        # shift input and mask\n",
        "        rolled_input_ids = torch.roll(\n",
        "            features[0][\"input_ids\"], random_shift_length\n",
        "        ).unsqueeze(0)\n",
        "        rolled_attention_mask = torch.roll(\n",
        "            features[0][\"attention_mask\"], random_shift_length\n",
        "        ).unsqueeze(0)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": rolled_input_ids,  # BS x SEQ_LEN\n",
        "            \"labels\": rolled_input_ids,  # BS x SEQ_LEN\n",
        "            \"attention_mask\": rolled_attention_mask,  # BS x SEQ_LEN\n",
        "        }"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6IfYJ5sy4V6"
      },
      "source": [
        "To instantiate the data collator the length of padded input_ids needs to be calculated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Levf0Jy4V6"
      },
      "source": [
        "# the non_padded_sequence_length defines the max shift for our data collator\n",
        "non_padded_sequence_length = sequence_length - sum(\n",
        "    dataset[\"attention_mask\"][0]\n",
        ")\n",
        "\n",
        "# get the data collator\n",
        "data_collator = ReformerCollator(non_padded_sequence_length)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scsiWWZ0y4V6"
      },
      "source": [
        "Next, we will define our reformer model by defining the ReformerConfig."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y9GBlUIy4V6"
      },
      "source": [
        "config = {\n",
        "    \"attention_head_size\": 64,\n",
        "    \"attn_layers\": [\"local\", \"lsh\", \"local\", \"lsh\", \"local\", \"lsh\"],\n",
        "    \"axial_pos_embds\": True,\n",
        "    \"sinusoidal_pos_embds\": False,\n",
        "    \"axial_pos_embds_dim\": [64, 192],\n",
        "    \"axial_pos_shape\": [512, 1024],\n",
        "    \"lsh_attn_chunk_length\": 64,\n",
        "    \"local_attn_chunk_length\": 64,\n",
        "    \"feed_forward_size\": 512,\n",
        "    \"hidden_act\": \"relu\",\n",
        "    \"hidden_size\": 256,\n",
        "    \"is_decoder\": True,\n",
        "    \"max_position_embeddings\": 524288,\n",
        "    \"num_attention_heads\": 2,\n",
        "    \"num_buckets\": [64, 128],\n",
        "    \"num_hashes\": 1,\n",
        "    \"vocab_size\": 320,\n",
        "    \"lsh_attention_probs_dropout_prob\": 0.0,\n",
        "    \"lsh_num_chunks_before\": 1,\n",
        "    \"lsh_num_chunks_after\": 0,\n",
        "    \"local_num_chunks_before\": 1,\n",
        "    \"local_num_chunks_after\": 0,\n",
        "    \"local_attention_probs_dropout_prob\": 0.025,\n",
        "    \"hidden_dropout_prob\": 0.025,\n",
        "}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_eqID2Gy4V6"
      },
      "source": [
        "config = ReformerConfig(**config)\n",
        "model = ReformerModelWithLMHead(config)\n",
        "model = model.train()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF6Q9V1by4V6"
      },
      "source": [
        "### Training args\n",
        "\n",
        "training_args = {\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"max_steps\": 100,\n",
        "    \"do_train\": True,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"logging_steps\": 1,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"per_device_train_batch_size\": 1,\n",
        "    \"per_device_eval_batch_size\": 1,\n",
        "    \"save_steps\": 1,\n",
        "    \"output_dir\": \"./\"\n",
        "}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDrPTT9_y4V6"
      },
      "source": [
        "training_args = TrainingArguments(**training_args)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpCaoXUzy4V7"
      },
      "source": [
        "### Accuracy metrics\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    non_padded_indices = (pred.label_ids != -100)\n",
        "\n",
        "    # correctly shift labels and pred as it's done in forward()\n",
        "    labels = pred.label_ids[..., 1:][non_padded_indices[..., 1:]]\n",
        "    pred = np.argmax(pred.predictions[:, :-1], axis=-1)[non_padded_indices[..., :-1]]\n",
        "\n",
        "    acc = np.mean(np.asarray(pred == labels), dtype=np.float)\n",
        "    return {\"accuracy\": acc}"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWrB0aVly4V7"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=dataset\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vhbjw8hEy4V7",
        "outputId": "7499275c-2465-4865-e1eb-d5baa98d0f80"
      },
      "source": [
        "try:\n",
        "  trainer.train()\n",
        "except KeyboardInterrupt:\n",
        "  print('Training was interrupted manually, last model saved will be used for prediction')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 56:03, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>5.868035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>5.867171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>5.866279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5.866044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>5.864196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>5.861832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>5.859360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>5.856157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>5.851445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>5.847889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>5.842067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>5.836717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>5.830495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>5.823853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>5.817585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>5.807805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>5.801331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>5.791651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>5.782852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.773883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>5.761784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>5.751844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>5.740507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>5.724978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>5.712065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>5.700606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>5.686087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>5.671885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>5.658353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>5.641746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>5.627614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>5.609538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>5.592724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>5.575801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>5.558612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.538750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>5.522519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>5.500863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>5.481163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.460881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>5.439087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>5.417431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>5.397629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.375240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>5.354935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>5.334908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>5.316357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.298959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>5.283628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>5.270875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>5.260211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.252412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>5.246136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>5.241162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>5.237631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.234780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>5.232109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>5.229699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>5.227177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.224522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>5.221493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>5.218273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>5.214939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>5.211858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>5.207933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>5.204751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>5.201240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>5.198455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>5.195867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>5.193875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>5.191870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>5.189891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>5.188794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>5.187670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>5.186256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>5.184961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>5.184471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>5.183055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>5.182037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>5.180586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>5.180111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>5.178793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>5.177962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>5.177326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>5.176587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>5.176036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>5.175338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>5.174502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>5.174345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>5.173625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>5.173151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>5.172823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>5.172768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>5.172027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>5.171949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>5.171380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>5.171487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>5.171201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>5.171212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.170841</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du7LYjC_qFuU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}